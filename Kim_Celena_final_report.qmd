---
title: "Predicting the Length of Comments on the Jubilee Middle Ground video 'Are We Allies? Black Americans vs Asian Americans'"
subtitle: |
  | Final Project 
  | Data Science 2 with R (STAT 301-2)
author: "Celena Kim"
date: today

format:
  html:
    toc: true
    embed-resources: true
    
execute:
  echo: false
  warning: false

from: markdown+emoji 
reference-location: margin
citation-location: margin
---

```{r}
#| label: basic-setup

# load packages ----
library(tidyverse)
library(tidymodels)
library(here)
library(doMC)
library(knitr)

# handle common conflicts
tidymodels_prefer()

# load data
allies <- read_rds("data/allies.rds")

# load training data/fits
load(here("data_splits/allies_split.rda"))
load(here("results/null_fit_a.rda"))
load(here("results/log_reg_fit_a.rda"))
load(here("results/log_reg_fit_b.rda"))
load(here("results/tuned_en_a.rda"))
load(here("results/tuned_en_b.rda"))
load(here("results/tuned_knn_a.rda"))
load(here("results/tuned_knn_b.rda"))
load(here("results/tuned_rf_a.rda"))
load(here("results/tuned_rf_b.rda"))
load(here("results/tuned_bt_a.rda"))
load(here("results/tuned_bt_b.rda"))
load(here("results/final_fit.rda"))
load(here("regression_attempt/results/reg_tbl_result.rda"))
```

::: {.callout-tip icon="false"}
## Github Repo Link

[Celena Kim Repo Link](https://github.com/stat301-2-2024-winter/final-project-2-celenakim)
:::

## Introduction

The data that I chose for my project was derived from gathering data on the psychological analysis of comments under the Jubilee video titled [Are We Allies? Black Americans vs Asian Americans \| Middle Ground](https://www.youtube.com/watch?v=pXo2ub_nZFc) found on [YouTube](https://www.youtube.com). This video features a poignant discussion between East Asian Americans and Black Americans, highlighting the conflict and tension present between these groups. As I am a part of the Social Cognition and Intergroup Processes Lab on campus, I thought it would be really interesting to combine social psychology with data science to explore topics on bias and race through quantitative analysis. Therefore, I decided to explore the comments under this video by utilizing the software LIWC[^1], a text analysis program that can categorize text by the overall emotion it conveys. Each comment under the video was analyzed and given a score of how much they communicated various emotions/categories such as anger, affiliation, power, assent, insight, sadness, etc. After creating this data set, I ended up with 15,397 observations of comments and 50 variables, 44 of which are my predictor variables.

[^1]: an explanation of how this software works can be found [here](https://www.liwc.app/help/howitworks)

My predictive research question is: **How long is a comment on the Jubilee video "Are We Allies? Black Americans vs Asian Americans \| Middle Ground"?**

This is a classification problem with the target variable being the length of a comment, either short or long. The predictors are the LIWC emotional categories, as well as a categorical variable of whether a comment contains positive emotion or not.

This prediction model is useful because it could provide insights into the factors of emotional response that influence the level of discussion between the viewers in response to watching the Jubilee video. For content creators such as Jubilee, this prediction could inform their strategies for knowing what type of content resonates with their audience in order to influence heavy engagement and conversation.

## Important Note-- Prediction Problem Type Switch
Originally, my final project was regression, predicting the likes of a comment with the same LIWC predictors. However, after running my models and obtaining the results for RMSE, I realized that the models were not efficient at predicting. This is most likely due to the fact that the distribution of the comment likes variable is so disproportionately skewed right, with a couple of extreme outliers in the thousands vs. the rest being mostly under 10 likes, that even setting a yeo-johnson transformation could barely help to normalize the distribution. Although I could have decided to just stick with my results and keep the problem type as regression, I wanted to create more efficient models through a classification problem, so I decided to make the change. My full regression project can be viewed in the regression attempt folder, and the resulting RMSEs can be viewed in the section "Appendix: Technical Info". From this, I have learned that predicting a variable with such a wide range of values such as likes was not the best idea, as even transforming the variable could not help to create successful models.

## Data Overview

### Response Variable Analysis

```{r}
#| label: fig-target-exploration
#| fig-cap: An exploration of the response variable, the length of a comment.

ggplot(allies, aes(x = comment_length)) +
  geom_bar() +
  labs(x = "Comment Length",
       y = "Count",
       title = "Distribution of the Length of Comments",
       subtitle = "The categories are relatively balanced.") +
  theme_minimal()
```

As seen in @fig-target-exploration, the distribution of comment length is relatively balanced between the long and short length categories.

### Data Inspection

```{r}
#| label: tbl-allies-inspection
#| tbl-cap: An inspection of the allies data set for missingness.

missing_counts <- colSums(is.na(allies))

missing_counts_df <- data.frame(Missing_Count = missing_counts)

kable(missing_counts_df)

```

After inspecting the allies data set for missingness, @tbl-allies-inspection shows that there only seems to be missingness in the ID of a comment's parent comment. However, this does not seem to pose a significant issue, as this variable is not going to be used for my analysis.

### Categorical Variable Inspection

```{r}
#| label: fig-cat-inspection
#| fig-cap: An inspection of the distribution of comments with/without positive emotion.

ggplot(allies, aes(x = pos_emo)) +
  geom_bar() +
  labs(x = "Presence of Positive Emotion or Not",
       y = "Count",
       title = "Distribution of the Presence of Positive Emotion Within Comments",
       subtitle = "The categories appear to be balanced.") +
  theme_minimal()
```

As visualized by @fig-cat-inspection, the distribution of comments with positive emotion vs. no positive emotion appear to be balanced.

### Predictor Variables Exploration

```{r}
#| label: fig-rest-raw-inspection
#| fig-cap: An example of the distributions of the numerical predictor variables.

allies_train_portion <- allies_train |> 
  slice_sample(prop = 0.8)

ggplot(allies_train_portion, aes(x = anger)) +
  geom_density() +
  labs(title = "Distribution of Anger",
       subtitle = "The variable is heavily skewed right",
       x = "Anger",
       y = "Density") +
  theme_minimal()
```

As shown in @fig-rest-raw-inspection, all the numerical predictor variables such as the presence of anger within comments are heavily skewed right. Log transformations will not work as these variables contain values of 0, and a square root transformation does not seem to help normalize their distributions. Thus, these variables will undergo a BoxCox transformation within a recipe step.

### Predictor Variable Relationships

```{r}
#| label: fig-cor-plot
#| fig-cap: A visualization of the correlations that exist between the predictor variables.

corr <- allies_train_portion |> 
  select(where(is.numeric)) |> 
  cor()

allies_cor <- ggcorrplot::ggcorrplot(corr) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, size = 4),
        axis.text.y = element_text(size = 4))

allies_cor
```

In order to determine possible relationships between the numerical predictor variables, I created a correlation plot. @fig-cor-plot shows that some of the variable pairs with the strongest positive correlations seem to be: focus present & verb, affiliation & drives, negative emotion & sadness, cognitive processes & certainty, and achieve & social. Identifying these relationships is useful for adding interaction terms to my recipes as they will be able to capture the joint effect of two variables on the outcome variable of comment likes.

### Highly Correlated Variables with the Target Variable
In the initial stages of my project, I was obtaining accuracies very close to/equal to 1, indicating that I may have included variables that were highly correlated with my target variable in my recipe steps. After reviewing the correlations of my target variable with my predictor value, I realized that I had included the variables of the word count of a comment and the average number of words per sentence for a comment, which are variables that are directly linked to the length of a comment, and thus would impact my accuracy. After removing those variables, the accuracies were much more reasonable. A table outlining the correlations between my target variable and predictors can be found in the "Appendix: Technical Info" section of this report.

## Methods
I implemented an 80-20 training-test split using stratified sampling (stratified by target variable, comment length, with 4 strata). The resamples were constructed by taking the training dataset and applying repeated V-fold cross-validation (10 folds, 5 repeats) with stratification on the target variable with 4 strata. The prediction problem is classification.

### Models
The following model types were specified with a plan for tuning hyperparameters using a regular grid:

-   1 Null model
-   2 Logistic Regression models (`lm` engine)
-   2 Elastic net models (`glmnet` engine)
    -   Mixture was explored over $[0,1]$ range with 5 levels
    -   Penalty was explored over $[-10,0]$ range with 5 levels
-   2 K-nearest neighbors models (`kknn` engine)
    -   Neighbors was explored over $[1,45]$ with 5 levels
-   2 Random forest models (`ranger` engine)
    -   Number of trees set to 1,000
    -   Number of randomly selected predictors to split on was explored over $[1, 30]$ with 5 levels
    -   Minimum number of data points in a node for splitting was explored over $[1,30]$ with 5 levels
-   2 Boosted tree models (`xgboost` engine)
    -   Number of trees was explored over $[100, 1000]$ with 4 levels
    -   Number of randomly selected predictors to split on was explored over $[1,30]$ with 5 levels
    -   Minimum number of data points in a node for splitting was explored over $[1,30]$ with 5 levels
    -   Learning rate was explored over $[-5,-0.2]$ with 5 levels
    
### Recipes
4 different recipes were created:

-   **Recipe 1: kitchen sink**: This is my basic recipe, with appropriate variables removed (the ID of a comment, the ID of a parent comment, the commenter's username, the comment itself, the variables of the word count and average words per sentence of a comment, as these variables were highly correlated with the target variable, and any variables with zero variance), numerical predictors transformed with a BoxCox transformation, factor variables dummy coded, and predictors centered/normalized.

-   **Recipe 2: kitchen sink (trees)**: This is the trees version of my basic recipe, with appropriate variables removed (the ID of a comment, the ID of a parent comment, the commenter's username, the comment itself, the variables of the word count and average words per sentence of a comment, as these variables were highly correlated with the target variable, and any variables with zero variance), factor variables one-hot encoded, and predictors centered/normalized.

-   **Recipe 3: transformed & interactions between selected variables**: This is my complex recipe, with numerical predictors transformed with a BoxCox transformation, factor variables dummy coded, predictors centered/normalized, and interactions between variables with high correlations between each other: focus present & verb, affiliation & drives, negative emotion & sadness, cognitive processes & certainty, and achieve & social.

-   **Recipe 4: transformed & selected variables (trees)**: This is the trees version of my complex recipe, only utilizing the variables included in recipe 3: focus present, verb, affiliation, drives, negative emotion, sadness, cognitive processes, certainty, achieve, and social. Numerical predictors were transformed with a BoxCox transformation, factor variables were dummy coded, and predictors were centered/normalized.

The null model used recipe 1.

The logistic regression, elastic net, and k nearest neighbors models each used recipes 1 and 3, to obtain a and b versions of each model.

The k-nearest neighbors, random forest, and boosted tree models used recipes 3 and 4, to obtain a and b versions of each model.

## Model Building & Selection

## Assessment Metric
I decided **accuracy** would be the metric by which I would compare models. Accuracy essentially measures how often each model correctly predicts the class labels of short or long comment lengths.

## Best Performing Model Results
```{r}
#| label: tbl-model-results
#| tbl-cap: A table of my models and their accuracies.

model_results <- as_workflow_set(
  null = null_fit_a,
  log_reg_a = log_reg_fit_a,
  log_reg_b = log_reg_fit_b,
  en_a = tuned_en_a,
  en_b = tuned_en_b,
  knn_a = tuned_knn_a,
  knn_b = tuned_knn_b,
  rf_a = tuned_rf_a,
  rf_b = tuned_rf_b,
  bt_a = tuned_bt_a,
  bt_b = tuned_bt_b)

tbl_result_accuracy <- model_results |> 
  collect_metrics() |> 
  filter(.metric == "accuracy") |> 
  slice_min(mean, by = wflow_id) |> 
  distinct(wflow_id, .keep_all = TRUE) |> 
  arrange(mean) |> 
  select(`Model Type` = wflow_id, 
         `Accuracy` = mean, 
         `Std Error` = std_err, 
         `Num Models` = n) |> 
  knitr::kable(digits = c(NA, 3, 6, 0))

tbl_result_accuracy
```
@tbl-model-results contains a table of the best performing model results, in terms of accuracy. The random forest model a (with the kitchen sink recipe) obtained the highest accuracy of 0.949, while the null model, elastic net model a, and elastic net model b all obtained the lowest accuracy of 0.513.

## Tuning Parameters
Here are the best-performing hyperparameters for each model that I tuned: elastic net, k nearest neighbors, random forest, and boosted tree models.
Visualizations of tuning parameters may be found in the "Appendix: Technical Info" section.

```{r}
#| label: tbl-selectbest-tuned-en-a
#| tbl-cap: The best tuning parameter values for the elastic net model a.

select_best(tuned_en_a, metric = "accuracy") |> 
  knitr::kable()
```
As seen in @tbl-selectbest-tuned-en-a, the best parameters to obtain the highest accuracy for the elastic net model a is a penalty of 0 and a mixture of 0.7625.

```{r}
#| label: tbl-selectbest-tuned-en-b
#| tbl-cap: The best tuning parameter values for the elastic net model b.

select_best(tuned_en_b, metric = "accuracy") |> 
  knitr::kable()
```
@tbl-selectbest-tuned-en-b show that the best parameters to obtain the highest accuracy for the elastic net model b are also a penalty of 0 and a mixture of 0.7625.

```{r}
#| label: tbl-selectbest-tuned-knn-a
#| tbl-cap: The best tuning parameter values for the k nearest neighbors model a.

select_best(tuned_knn_a, metric = "accuracy") |> 
  knitr::kable()
```
@tbl-selectbest-tuned-knn-a shows that the best parameter to obtain the highest accuracy for the k nearest neighbors model a is 1 neighbor.

```{r}
#| label: tbl-selectbest-tuned-knn-b
#| tbl-cap: The best tuning parameter values for the k nearest neighbors model b.

select_best(tuned_knn_b, metric = "accuracy") |> 
  knitr::kable()
```
As seen in @tbl-selectbest-tuned-knn-b, the best parameter to obtain the highest accuracy for the k nearest neighbors model b is 34 neighbors.

```{r}
#| label: tbl-selectbest-tuned-rf-a
#| tbl-cap: The best tuning parameter values for the random forest model a.

select_best(tuned_rf_a, metric = "accuracy") |> 
  knitr::kable()
```
@tbl-selectbest-tuned-rf-a shows that the best parameters to obtain the highest accuracy for the random forest model a is 15 variables randomly sampled at each split and a minimum of 2 samples required to be present in a node for further splitting to be considered.

```{r}
#| label: tbl-selectbest-tuned-rf-b
#| tbl-cap: The best tuning parameter values for the random forest model b.

select_best(tuned_rf_b, metric = "accuracy") |> 
  knitr::kable()
```
@tbl-selectbest-tuned-rf-b shows that the best parameters to obtain the highest accuracy for the random forest model b is 8 variables randomly sampled at each split and a minimum of 2 samples required to be present in a node for further splitting to be considered.

```{r}
#| label: tbl-selectbest-tuned-bt-a
#| tbl-cap: The best tuning parameter values for the boosted tree model a.

select_best(tuned_bt_a, metric = "accuracy") |> 
  knitr::kable()
```
As seen in @tbl-selectbest-tuned-bt-a, the best parameters to obtain the highest accuracy for the boosted tree model a is 1 variable randomly sampled at each split, a minimum of 2 samples required to be present in a node for further splitting to be considered, and a learning rate of 0.631.

```{r}
#| label: tbl-selectbest-tuned-bt-b
#| tbl-cap: The best tuning parameter values for the boosted tree model b.

select_best(tuned_bt_b, metric = "accuracy") |> 
  knitr::kable()
```
Finally, as visualized by @tbl-selectbest-tuned-bt-b, the best parameters to obtain the highest accuracy for the boosted tree model a is 15 variables randomly sampled at each split, a minimum of 2 samples required to be present in a node for further splitting to be considered, and a learning rate of 0.631.

## Further Tuning?
The tuning parameters seemed to produce high performing models, although further tuning could be explored in case there was overfitting that needed to be reduced in future explorations.

## Comparison of Model Type/Recipe Performance
The elastic net model was the only model type where the accuracy value was the same for both the a and b versions of the model. The elastic net models also obtained the same accuracy value as the null model, showing that the elastic net models were no better than a baseline model. For the k nearest neighbors and logistic regression models, the models with the more "complex" recipe (interaction terms and BoxCox transformations on the predictors) obtained a slightly higher accuracy than the models with the kitchen sink recipe, but these accuracy values were still very similar to each other within the model types: 0.821 vs. 0.825 for k nearest neighbors models a vs. b, and 0.731 vs. 0.741 for logistic regression models a vs. b. Thus, there is not that notable of a difference between the performance of the kitchen sink vs. complex recipes within those model types. On the other hand, for the random forest and boosted tree models, the models with the kitchen sink recipe performed better than the models with the more "complex" recipe to a greater degree (0.903 vs. 0.840 for boosted tree model a vs. b, and 0.949 vs. 0.908 for random forest model a vs. b). It was interesting to see that the tree models performed better with the kitchen sink recipe, while the logistic regression and k nearest neighbors models performed better with the more complex recipe.

## Winning Model
My winning model was my **random forest model a**, with the basic kitchen sink recipe, and the accuracy was **0.949**, or **94.9%**. It was surprising that the model with the kitchen sink recipe won, as I had predicted that a model with one of my complex recipes would win as those recipes included interaction terms with highly correlated variables. However, it could be that those additional interaction steps overcomplicated the model, thus rendering the basic kitchen sink recipe to perform more efficiently. 

## Final Model Analysis

### Accuracy
```{r}
#| label: fig-finalfit-accuracy
#| fig-cap: The accuracy value for my final winning random forest model a.

predict_rf <- allies_test |> 
  select(comment_length) |> 
  bind_cols(predict(final_fit, allies_test))

accuracy_rf <- accuracy(predict_rf, 
                               truth = comment_length, 
                               estimate = .pred_class)

accuracy_rf |> 
  knitr::kable()
```
@fig-finalfit-accuracy shows that after fitting the final winning random forest model a to the testing data, I obtained an accuracy of 0.993. This is a very high accuracy value, indicating that approximately 99.3% of the predictions made by the model match the actual lengths of comments in the testing dataset. This suggests that the random forest model a is performing very well in predicting the length of comments.

### Confusion Matrix
```{r}
#| label: fig-conf-mat
#| fig-cap: A confusion matrix of the comment length predictions for the final random forest a winning model. 

conf_mat_rf<- conf_mat(predict_rf, 
                              truth = comment_length, 
                              estimate = .pred_class) 

conf_mat_rf_df <- as.data.frame.matrix(conf_mat_rf$table)

knitr::kable(conf_mat_rf_df)
```
The confusion matrix in @fig-conf-mat is a representation of the performance of a classification model, with True Positives, True Negatives, False Positives, and False Negatives. In this case, my winning model correctly predicted that there were 1558 long comments, the model correctly predicted that there were 1499 short comments, the model incorrectly predicted that 22 comments were short when they were actually long, and the model incorrectly predicted that 1 comment was long when it was actually short.

### ROC Curve
```{r}
#| label: fig-roc-curve
#| fig-cap: A visualization of the performance of the final winning random forest a model.

allies_predicted_prob_rf <- allies_test |> 
  select(comment_length) |> 
  bind_cols(predict(final_fit, 
                    allies_test, 
                    type = "prob"))

allies_rf_curve <- roc_curve(allies_predicted_prob_rf, 
                             comment_length, 
                                   .pred_long)

autoplot(allies_rf_curve)
```
The receiver operating characteristic (ROC) curve in @fig-roc-curve is a graphical representation of the performance of the winning random forest a model. If the curve was close to the diagonal line, then my modelâ€™s predictions would be unsuccessful and no better than random guessing. However, since the curve is very close to the top left-hand corner, I can conclude that my model performs very well at various thresholds.

### ROC AUC
```{r}
#| label: fig-roc-auc
#| fig-cap: The area under the ROC curve of the winning random forest a model.

roc_auc(allies_predicted_prob_rf, 
        comment_length, 
        .pred_long) |> 
  knitr::kable()
```
@fig-roc-auc computes the area under the ROC curve shown previously. With a value of 0.999, this suggests that the random forest a model has very high performance in terms of classification accuracy.

### Did the effort pay off?
After assessing my final model's performance, it was very successful, and I do think that the effort of building a predictive model paid off. Compared to the baseline model (before choosing the final model), the accuracy of random forest model a was approximately 85.09% higher than the null, indicating a notable improvement in model performance. The recipes used for these models were both basic kitchen sink recipes, so this model improvement was likely not the result of an improved recipe. Rather, it could be attributed to the fact that the random forest model itself leverages more advanced techniques than the null to carry out these predictions, thus resulting in a higher accuracy value.

## Conclusion
Overall, it appears that the random forest model with the basic kitchen sink recipe performed exceptionally well, while the null and elastic net models performed the worst. In terms of recipe engineering, simplicity proved to be the most successful, as the basic kitchen sink recipe version of the random forest model obtained a higher accuracy than the complex recipe version. With this random forest model a, the length of a comment under the Jubilee video "Are We Allies" of being either short or long can be predicted with a very high accuracy.

Moreover, I am glad that I decided to switch my project from regression to classification, as I was able to create exponentially more successful models than compared to the regression results. Although I could have just kept my regression problem as my main final report and explained why those models did not perform well, I think that switching over to the classification models taught me how to be resilient in the face of obtaining results that weren't successful and flexible in rerouting to devise another approach. I also have a much clearer understanding of what variables/components would constitute successful model results for both classification and regression problems, in addition to having obtained great practice with both prediction type problems. In the future, I would like to have a second try at tackling a different type of regression problem and obtain more successful results than I had with my original report. For instance, I could possibly predict a certain LIWC emotional category such as level of positive emotion or level of negative emotion based on the other LIWC components present within a comment. Now that I have a better understanding of regression problems, I think this new attempt would result in much greater success.

## References

"Are We Allies? Black Americans vs Asian Americans \| Middle Ground" *YouTube*, uploaded by Jubilee, 17 January 2021, <https://www.youtube.com/watch?v=pXo2ub_nZFc>.

LIWC, <https://www.liwc.app/>.

## Appendix: Technical Info
### Original Regression Model Results
```{r}
#| label: tbl-orig-reg-results
#| tbl-cap: The RMSE results from my original project, a regression problem predicting the likes on a comment.

reg_tbl_result
```
@tbl-orig-reg-results shows the RMSEs (on a Yeo-Johnson scale) obtained from my original project, a regression type problem of predicting the number of likes on a comment. As can be seen from the RMSEs, there is not a notable difference in RMSE performance between the null model and the best model, the random forest a model. Thus, my best model was barely better than the simplest possible model/prediction strategy possible, indicating that it was not that successful. After obtaining such unpromising results, I decided to switch to a classification prediction problem of predicting comment length. 

###  Highly Correlated Variables with the Target Variable Table
```{r}
#| label: tbl-target-var-cors
#| tbl-cap: A table of the correlations between the target variable of comment length with the numerical predictor variables.

cor_set <- allies_train |> 
  select(comment_length, where(is.numeric))

correlation <- cor(cor_set[, -which(names(cor_set) == "comment_length")], 
                   as.numeric(cor_set$comment_length), 
                   use = "pairwise.complete.obs")

correlation_tbl <- correlation |> 
  enframe() |> 
  arrange(value) |> 
  knitr::kable()

correlation_tbl
```
As seen in @tbl-target-var-cors, the predictors of the word count of a comment and the average words per sentence of a comment have the strongest correlations with the target variable of comment length, as those are both factors that directly impact/have a direct relationship with how long a comment would be. Thus, it was necessary to remove those predictors from my recipes.

### Visualizations of Tuning Parameters
```{r}
#| label: fig-autoplot-tuned-en-a
#| fig-cap: A visualization of the best tuning parameters for the elastic net model a.

autoplot(tuned_en_a)
```
As seen in @fig-autoplot-tuned-en-a the best parameters to obtain the highest accuracy for the elastic net model a is a penalty of 0 and a mixture of 0.7625.

```{r}
#| label: fig-autoplot-tuned-en-b
#| fig-cap: A visualization of the best tuning parameters for the elastic net model b.

autoplot(tuned_en_b)
```
@fig-autoplot-tuned-en-b shows that the best parameters to obtain the highest accuracy for the elastic net model b are also a penalty of 0 and a mixture of 0.7625.

```{r}
#| label: fig-autoplot-tuned-knn-a
#| fig-cap: A visualization of the best tuning parameters for the k nearest neighbors model a.

autoplot(tuned_knn_a)
```
@fig-autoplot-tuned-knn-a shows that the best parameter to obtain the highest accuracy for the k nearest neighbors model a is 1 neighbor.

```{r}
#| label: fig-autoplot-tuned-knn-b
#| fig-cap: A visualization of the best tuning parameters for the k nearest neighbors model b.

autoplot(tuned_knn_b)
```
As seen in @fig-autoplot-tuned-knn-b, the best parameter to obtain the highest accuracy for the k nearest neighbors model b is 34 neighbors.

```{r}
#| label: fig-autoplot-tuned-rf-a
#| fig-cap: A visualization of the best tuning parameters for the random forest model a.

autoplot(tuned_rf_a)
```
@fig-autoplot-tuned-rf-a shows that the best parameters to obtain the highest accuracy for the random forest model a is 15 variables randomly sampled at each split and a minimum of 2 samples required to be present in a node for further splitting to be considered.

```{r}
#| label: fig-autoplot-tuned-rf-b
#| fig-cap: A visualization of the best tuning parameters for the random forest model b.

autoplot(tuned_rf_b)
```
@fig-autoplot-tuned-rf-b shows that the best parameters to obtain the highest accuracy for the random forest model b is 8 variables randomly sampled at each split and a minimum of 2 samples required to be present in a node for further splitting to be considered.

```{r}
#| label: fig-autoplot-tuned-bt-a
#| fig-cap: A visualization of the best tuning parameters for the boosted tree model a.

autoplot(tuned_bt_a)
```
As seen in @fig-autoplot-tuned-bt-a, the best parameters to obtain the highest accuracy for the boosted tree model a is 1 variable randomly sampled at each split, a minimum of 2 samples required to be present in a node for further splitting to be considered, and a learning rate of 0.631.

```{r}
#| label: fig-autoplot-tuned-bt-b
#| fig-cap: A visualization of the best tuning parameters for the boosted tree model b.

autoplot(tuned_bt_b)
```
Finally, as visualized by @fig-autoplot-tuned-bt-b, the best parameters to obtain the highest accuracy for the boosted tree model a is 15 variables randomly sampled at each split, a minimum of 2 samples required to be present in a node for further splitting to be considered, and a learning rate of 0.631.